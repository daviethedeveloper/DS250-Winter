---
title: "Machine Learning Report Homes"
subtitle: "Course DS 250"
author: "David Pineda"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
    
---

## [Libraries Used In Report]

```{python}
#| label: libraries
#| include: false
from types import GeneratorType
import pandas as pd
import altair as alt
import numpy as np
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn import tree
```


## [Data Imported In Report]

```{python}

#  - Data being read and set to specific variables - 
dwellings_denver = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_denver/dwellings_denver.csv")

dwellings_ml = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv")

dwellings_neighborhoods = pd.read_csv("https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv")

```






# [Charting Relationship - Homes Before 1980]

Below, I created 2 charts to evaluate potential relationships between the home variables and before1980. This will help determine from the different features, to predict a home that was built before or after 1980.  

```{python}
h_subset = dwellings_ml.filter(['livearea', 'finbsmnt', 
    'basement', 'yearbuilt', 'nocars', 'numbdrm', 'numbaths', 
    'stories', 'yrbuilt', 'before1980']).sample(500)

# sns.pairplot(h_subset, hue = 'before1980')


```



### [Number Bedrooms - Year Built]

Based on these observations, a machine learning algorithm could use the number of bedrooms and the year built as features to predict the price of a house. The algorithm could also use the binary feature indicating whether the house was built before or after 1980 as a categorical feature to improve its prediction accuracy.

```{python}
#| label: Q1
# Chart 1

# The given code generates a scatter plot using the Altair library in Python. The plot shows the relationship between the number of bedrooms and the year built for a subset of housing data. The x-axis represents the number of bedrooms, and the y-axis represents the year built. The points on the plot represent individual houses, and the color of each point indicates whether the house was built before or after 1980.

(alt.Chart(h_subset)
.mark_point() 
.encode(
    x=alt.X('yrbuilt', axis=alt.Axis(format=".0f"), scale=alt.Scale(domain=[1880, 2200])),
    y=alt.Y('numbdrm'),
    
    color = 'before1980:N'
)
)



```

### [Number Baths - Year Built]

Based on these observations, a machine learning algorithm could use the number of baths and the year built as features to predict the house was built before or after 1980 as a categorical feature to improve its prediction accuracy.


```{python}
# Chart 2
# The plot shows the relationship between the number of baths and the year built for a subset of housing data. The x-axis represents the number of baths, and the y-axis represents the year built. The points on the plot represent individual houses, and the color of each point indicates whether the house was built before or after 1980.

(alt.Chart(h_subset)
.mark_point() 
.encode(
    x=alt.X('yrbuilt', axis=alt.Axis(format=".0f"), scale=alt.Scale(domain=[1880, 2200])),
    y=alt.Y('numbaths'),
    color = 'before1980:N'
)
)


```







## [Classification Model - Homes before 1980]

The following is a classification model for labeled houses built "before 1980" or "during or after 1980". The accuracy scored here on this model is:  91% . The model choice presented is a decision tree due to helpful evalutation of options. The decision tree helped with possible outcomes of choosing which options it might be. The parameters used in this test were filterd out with the "yrbuilt", "parcel", "before1980" - model will truly predict if a home is built before 1980. 

```{python}
#| label: Q2

#
# Separate the features (X) and targets (Y)
x = dwellings_ml.drop(["yrbuilt", "parcel", "before1980"],axis=1)
y = dwellings_ml[["before1980"]]

# adding on the neighborhood data
#x2 = x.join(dwellings_neighborhoods, how='left')



# Split the data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)

#
# Create a decision tree
classifier_DT = DecisionTreeClassifier(max_depth = 11)

# Fit the decision tree
classifier_DT.fit(x_train, y_train)

# Test the decision tree (make predictions)
y_predicted_DT = classifier_DT.predict(x_test)


```



## [Plotting Feature Importance]

In my classification model, the features I excluded were the yearbuilt, parcel and before1980. This decision is based off the results that can increase my model to predict 100%. So, to preven my model reaching 100% accuracy, I excluded these features.

As seen in my printed features, the top five features are printed. The feature with the most importance is 'arcstyle_ONE-store' with  .30. From this result, I can conclude 'arcstyle_ONE-STORY' will be helpful feature to know if homes were built before 1980.


```{python}
#| label: Q3


feature_df = pd.DataFrame({'features':x.columns, 'importance':classifier_DT.feature_importances_})

# order by importance - limit to only 5
print(feature_df.sort_values('importance', ascending=False).head())

```


## [Quality of Classification model]

Balanced Accuracy is the average of sensitivity and specificity score. Since our data might be out balanced, the balanced accuracy will take into account the true positives balanced with true negatives. This way my classification model can be balanced out. The accuracy metric will take will take into account the true positives with the true negatives by the total of all the values in the classfication mode. 


```{python}
#| label: Q4

# Evaluates my classification model with Accruracy and Balanced Accurarcy

print("Accuracy:", metrics.accuracy_score(y_test, y_predicted_DT))
print("Balanced Accuracy:", metrics.balanced_accuracy_score(y_test, y_predicted_DT))


fi_df = feature_df.sort_values('importance', ascending=False).head()
alt.Chart(fi_df).mark_bar().encode(
    y='features',
    x='importance',
)

```